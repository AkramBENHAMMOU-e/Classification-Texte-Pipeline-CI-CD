\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{xcolor}
\usepackage{tcolorbox}

\geometry{margin=2.5cm}

% Couleurs personnalisées
\definecolor{ensetblue}{RGB}{0,51,102}
\definecolor{lightgray}{RGB}{245,245,245}

% En-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small{Classification de Texte avec Pipeline CI/CD Complet}}
\fancyhead[R]{\small \thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\fancyfoot[C]{\small{DevOps \& MLOps}}
\renewcommand{\footrulewidth}{0.4pt}

% Liens sans cadres colorés
\hypersetup{
  hidelinks
}

\title{Classification de texte avec pipeline CI/CD complet\\\small{Rapport technique}}
\author{Akram Benhammou \and Oussama Khouya}
\date{\today}

\begin{document}

%========================
% Page de garde
%========================
\begin{titlepage}
  \thispagestyle{empty}
  \begin{center}
    \vspace*{0.5cm}

    % Logo ENSET
    \includegraphics[width=4cm]{enset.png}\\[0.8cm]

    {\Large \textbf{ENSET Mohammedia}}\\[0.3cm]
    {\large Université Hassan II de Casablanca}\\[1.2cm]

    {\huge \textbf{Classification de texte avec pipeline CI/CD complet}}\\[0.5cm]
    {\Large \textcolor{ensetblue}{Rapport technique}}\\[1.5cm]

    \includegraphics[width=0.9\textwidth]{./snapshots/page-garde.png}\\[1.5cm]

    {\large \textbf{Réalisé par}}\\[0.4cm]
    {\large Akram Benhammou}\\
    {\large Oussama Khouya}\\[1.2cm]

    % Encadré par - avec boîte
    {\large \textbf{Encadré par}}\\[0.4cm]
    \fcolorbox{ensetblue}{lightgray}{%
      \parbox{0.6\textwidth}{\centering\large\textbf{Professeur Soufiane HAMIDA}}%
    }\\[1.5cm]

    {\large Année universitaire 2024--2025}\\[0.3cm]
    {\large \today}
  \end{center}
\end{titlepage}

%========================
% Résumé
%========================
\section*{Résumé}

Ce rapport présente la conception et la mise en place d'une chaîne MLOps complète pour un problème de classification de texte sur le jeu de données \emph{20 Newsgroups}. Le pipeline développé couvre l'ensemble du cycle de vie d'un modèle de machine learning, depuis le prétraitement linguistique des données textuelles jusqu'au déploiement automatisé en production. 

La solution proposée s'articule autour d'un modèle de classification basé sur la vectorisation TF--IDF combinée à un algorithme Random Forest, dont les performances sont suivies et tracées grâce à MLflow. Les artefacts générés (modèle entraîné, vectoriseur, métriques) sont ensuite packagés dans une image Docker exposant une API FastAPI. L'ensemble du processus est orchestré par des workflows GitHub Actions qui automatisent les tests, la construction de l'image, la génération de rapports CML et un scénario de déploiement progressif \emph{staging} vers \emph{production} avec mécanisme de rollback en cas d'échec.

L'objectif principal de ce travail est d'illustrer, de manière concrète et pédagogique, les bonnes pratiques DevOps et MLOps appliquées à un cas d'usage NLP réaliste.

\clearpage

\tableofcontents
\clearpage

%========================
% 1. Introduction
%========================
\section{Introduction}

\subsection{Contexte et problématique}

Ce projet s'inscrit dans le cadre du module \emph{DevOps \& MLOps} et correspond au Projet~9 intitulé \og Classification de Texte avec Pipeline CI/CD Complet \fg{}. Le scénario métier envisagé est celui d'un journal en ligne qui souhaite automatiser la catégorisation de ses articles en fonction de leur contenu textuel. Cette automatisation vise à faciliter la navigation des lecteurs par rubrique, à améliorer les systèmes de recommandation et à permettre une analyse éditoriale plus fine de la répartition thématique des publications.

Pour répondre à cette problématique, nous avons mis en place une chaîne MLOps complète qui prend en charge l'ensemble du cycle de vie du modèle de classification. Cette chaîne couvre le prétraitement des données textuelles, l'entraînement et le suivi expérimental du modèle, son packaging dans une image Docker, sa mise à disposition via une API REST, ainsi que l'automatisation de tous ces processus grâce à des pipelines CI/CD GitHub Actions. Le flux de déploiement intègre également la génération de rapports automatiques CML et une stratégie de promotion progressive de l'environnement \emph{staging} vers la \emph{production}.

\subsection{Objectifs du projet}

Les objectifs de ce projet sont doubles et complémentaires. D'un point de vue \textbf{Machine Learning}, il s'agit de construire un modèle de classification de texte performant capable de distinguer vingt catégories thématiques différentes. Ce modèle doit garantir une cohérence parfaite entre le prétraitement appliqué lors de l'entraînement et celui utilisé lors de l'inférence, condition indispensable pour obtenir des prédictions fiables en production. Les métriques visées incluent une accuracy satisfaisante ainsi qu'un F1-score pondéré reflétant les performances sur l'ensemble des classes.

D'un point de vue \textbf{DevOps et MLOps}, l'ambition est de mettre en place une infrastructure d'intégration et de déploiement continus robuste et reproductible. Cela implique l'automatisation des tests unitaires et d'intégration, la traçabilité complète des expériences grâce à MLflow, la génération de rapports de performance directement dans les pull requests via CML, et l'orchestration d'un déploiement progressif avec la possibilité de revenir à une version précédente en cas de problème.

\subsection{Périmètre et livrables}

Le périmètre de ce projet englobe l'ensemble de la chaîne de traitement, depuis la récupération des données textuelles brutes jusqu'au déploiement d'un service d'inférence containerisé et accessible via une API REST.

Les livrables produits comprennent un dépôt Git structuré contenant le code source, les scripts d'entraînement et de prétraitement, les tests automatisés et les workflows GitHub Actions. Le projet fournit également un pipeline de données reproduisible permettant le téléchargement, le nettoyage, le prétraitement linguistique et le découpage du jeu de données en ensembles d'entraînement, de validation et de test. Un modèle de classification accompagné de ses artefacts (vectoriseur TF--IDF, métriques de performance, rapports détaillés) est généré et versionné. Enfin, une API FastAPI est packagée dans une image Docker et intégrée à des pipelines CI/CD complets permettant les tests, le build et push de l'image, la génération de rapports CML et le déploiement automatisé avec mécanisme de rollback.

%========================
% 2. Description fonctionnelle et données
%========================
\section{Description fonctionnelle et données}

\subsection{Cas d'usage métier}

Le contexte métier de ce projet est celui d'un journal en ligne souhaitant automatiser la catégorisation de ses articles. Chaque texte publié doit être automatiquement attribué à une catégorie thématique parmi une vingtaine de possibilités : informatique, sport, politique, religion, sciences, etc.

Cette classification automatique apporte plusieurs bénéfices concrets pour l'organisation. Elle améliore significativement l'expérience utilisateur en permettant une navigation fluide par rubrique et en alimentant des systèmes de recommandation personnalisés. Elle aide également la rédaction à analyser la répartition des contenus publiés et à identifier d'éventuels déséquilibres thématiques. Enfin, elle prépare le terrain pour des cas d'usage plus avancés comme la recherche sémantique ou la personnalisation du contenu selon les préférences des lecteurs.

\subsection{Jeu de données et splits}

La solution implémentée s'appuie sur le jeu de données public \emph{20 Newsgroups}, un corpus classique en traitement du langage naturel constitué d'environ 20~000 documents issus de groupes de discussion Usenet. Chaque document est un texte en anglais (email ou article) associé à l'un des vingt groupes thématiques du corpus.

Le chargement des données est effectué via la fonction \texttt{sklearn.datasets.fetch\_20newsgroups} de scikit-learn. Pour limiter les biais liés aux métadonnées, les en-têtes, pieds de page et citations sont systématiquement supprimés lors du téléchargement, ne conservant que le corps du texte.

Le pipeline de préparation découpe ensuite le dataset en trois ensembles distincts selon une stratification sur la variable cible. L'ensemble d'entraînement représente 70\% des données et sert à ajuster les paramètres du modèle. L'ensemble de validation, comptant 15\% des données, permet le réglage des hyperparamètres et la comparaison de différentes configurations. Enfin, l'ensemble de test, également de 15\%, est strictement réservé à l'évaluation finale du modèle et n'est jamais utilisé pendant la phase de développement.

\subsection{Contraintes et critères de succès}

Plusieurs critères définissent le succès de ce projet. Sur le plan des performances, le modèle doit atteindre une accuracy et un F1-score pondéré significatifs sur le jeu de test, témoignant de sa capacité à généraliser correctement sur des données non vues pendant l'entraînement.

Sur le plan de la cohérence, le prétraitement NLP appliqué lors de l'entraînement doit être rigoureusement identique à celui utilisé par le service d'inférence, garantissant ainsi la reproductibilité des prédictions. Les pipelines CI/CD doivent être capables de reconstruire le modèle de manière automatique, d'exécuter l'ensemble des tests et de déployer un service à jour sans intervention manuelle. Enfin, l'infrastructure doit permettre un diagnostic facile des performances grâce aux rapports MLflow et CML générés automatiquement dans les pull requests.

%========================
% 3. Architecture globale MLOps
%========================
\section{Architecture globale MLOps}

\subsection{Vue d'ensemble}

L'architecture globale du projet suit un flux de bout en bout pensé pour maximiser la reproductibilité et l'automatisation. Ce flux débute par la préparation des données textuelles, incluant leur téléchargement depuis la source, leur nettoyage, leur prétraitement linguistique et leur découpage en ensembles d'entraînement, validation et test.

Une fois les données prêtes, la phase d'entraînement du modèle de classification s'enclenche, suivie immédiatement par une évaluation rigoureuse sur le jeu de test. L'ensemble de l'expérience est tracé dans MLflow, qui enregistre les hyperparamètres utilisés, les métriques obtenues et les artefacts générés. Ces artefacts, comprenant le modèle sérialisé et le vectoriseur TF--IDF, sont exportés dans un répertoire dédié pour être ensuite intégrés à l'image Docker.

Le service d'inférence, exposant une API FastAPI, est packagé dans cette image Docker. L'orchestration finale est assurée par des workflows GitHub Actions qui automatisent les tests, la construction et la publication de l'image, la génération de rapports CML et le déploiement progressif en staging puis en production, avec un mécanisme de rollback en cas d'échec.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{architecture globale-systeme.png}
  \caption{Architecture globale du système MLOps}
\end{figure}

\subsection{Organisation des répertoires}

La structure du dépôt Git a été pensée pour séparer clairement les différentes responsabilités et faciliter la maintenance du projet.

Le répertoire \texttt{src/} contient le code source Python principal. On y trouve \texttt{preprocess.py} qui gère le téléchargement et la préparation des données, \texttt{train.py} qui orchestre l'entraînement du modèle avec le logging MLflow et la sauvegarde des artefacts, \texttt{app.py} qui implémente l'API FastAPI avec ses endpoints \texttt{/health} et \texttt{/predict}, ainsi que \texttt{predict.py} offrant une interface en ligne de commande pour effectuer des prédictions ponctuelles.

Le répertoire \texttt{data/processed/} accueille les données prétraitées sous forme de fichiers CSV : \texttt{train.csv}, \texttt{validation.csv} et \texttt{test.csv}. Le répertoire \texttt{models/} stocke les artefacts du modèle, notamment \texttt{model.joblib} et \texttt{tfidf\_vectorizer.joblib}. Le répertoire \texttt{reports/} contient les résultats d'évaluation : \texttt{metrics.json}, \texttt{classification\_report.txt} et la matrice de confusion \texttt{confusion\_matrix.png}.

Les tests automatisés sont regroupés dans \texttt{tests/}, tandis que les pipelines CI/CD sont définis dans \texttt{.github/workflows/} avec les fichiers \texttt{docker.yaml}, \texttt{cml.yaml} et \texttt{deploy.yaml}. Le \texttt{Dockerfile} à la racine définit l'image de service pour l'API.

%========================
% 4. Pipeline de données et modèle NLP
%========================
\section{Pipeline de données et modèle NLP}

\subsection{Préparation des données}

Le script \texttt{src/preprocess.py} implémente un pipeline de préparation rigoureux en plusieurs étapes. La première étape consiste à télécharger le dataset \emph{20 Newsgroups} via scikit-learn, en prenant soin de supprimer les en-têtes, pieds de page et citations pour limiter les biais liés aux métadonnées.

Les données brutes sont ensuite structurées dans un DataFrame pandas avec deux colonnes : \texttt{text} contenant le texte original et \texttt{target} contenant l'identifiant de la classe. Un nettoyage simple est appliqué via la fonction \texttt{clean\_text} qui convertit le texte en minuscules, supprime la ponctuation, les chiffres et les espaces superflus.

Le traitement linguistique approfondi est réalisé par la fonction \texttt{process\_text} qui utilise NLTK pour tokeniser le texte, supprimer les stopwords anglais et appliquer une lemmatisation via le \texttt{WordNetLemmatizer}. Les exemples dont le texte prétraité est vide après ces opérations sont filtrés et exclus du dataset.

Enfin, le découpage en ensembles d'entraînement, validation et test est effectué avec \texttt{train\_test\_split} en respectant une stratification sur la variable cible. Les trois ensembles résultants sont sauvegardés au format CSV dans le répertoire \texttt{data/processed/}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{snapshots/pipeline.png}
  \caption{Diagramme du pipeline de préparation des données}
\end{figure}

\subsection{Modélisation et entraînement}

Le script \texttt{src/train.py} orchestre la phase d'entraînement du modèle. Il commence par charger les fichiers \texttt{train.csv} et \texttt{test.csv} générés par le pipeline de préparation, puis effectue un nettoyage des éventuelles valeurs manquantes sur la colonne \texttt{processed\_text}.

La vectorisation des textes est réalisée grâce au \texttt{TfidfVectorizer} de scikit-learn, configuré avec un nombre maximal de caractéristiques initialement fixé à 5000, mais ajustable selon les besoins. Cette représentation TF--IDF transforme chaque texte en un vecteur numérique pondéré par la fréquence des termes et leur spécificité dans le corpus.

Le modèle de classification choisi est un \texttt{RandomForestClassifier} paramétré avec 100 arbres de décision, une graine aléatoire fixée à 42 pour la reproductibilité et l'utilisation de tous les cœurs disponibles via \texttt{n\_jobs=-1}. L'évaluation sur le jeu de test produit les métriques d'accuracy, précision, rappel et F1-score pondérés, accompagnées d'un rapport détaillé par classe et d'une matrice de confusion.

Tous ces résultats sont sauvegardés localement : \texttt{metrics.json} pour les métriques globales, \texttt{classification\_report.txt} pour le rapport détaillé et \texttt{confusion\_matrix.png} pour la visualisation des erreurs. Les artefacts du modèle (\texttt{model.joblib} et \texttt{tfidf\_vectorizer.joblib}) sont également persistés dans le répertoire \texttt{models/}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{snapshots/train_eval.png}
  \caption{Diagramme du pipeline d'entraînement et d'évaluation du modèle}
\end{figure}

\subsection{Évaluation et validation du modèle}

Les performances du modèle sont évaluées principalement à travers l'accuracy globale et le F1-score pondéré, ce dernier étant particulièrement pertinent pour des problèmes multi-classes où certaines catégories peuvent être sous-représentées. La matrice de confusion offre une vue détaillée des confusions entre classes, permettant d'identifier les catégories que le modèle a du mal à distinguer.

Ces métriques servent de base pour comparer différentes configurations d'hyperparamètres, comme la valeur de \texttt{max\_features} du vectoriseur ou le nombre d'arbres de la forêt aléatoire. Elles permettent également de définir des seuils minimums de performance à respecter avant tout déploiement en production, garantissant ainsi un niveau de qualité acceptable. Enfin, elles alimentent les rapports CML générés automatiquement dans les pull requests, offrant une visibilité immédiate sur l'impact de chaque modification du code sur les performances du modèle.

%========================
% 5. Suivi expérimental et gestion des modèles
%========================
\section{Suivi expérimental et gestion des modèles}

\subsection{Tracking des expériences avec MLflow}

L'intégralité du processus expérimental est instrumentée avec MLflow, un outil open source dédié à la gestion du cycle de vie des modèles de machine learning. La configuration utilise un backend de stockage local via l'appel \texttt{mlflow.set\_tracking\_uri("file:./mlruns")}, ce qui permet de conserver l'historique des expériences directement dans le dépôt Git.

Chaque exécution du script d'entraînement crée un nouveau run MLflow regroupé sous l'expérience \texttt{Text\_Classification\_Projet9}. Les hyperparamètres clés comme \texttt{max\_features} et \texttt{n\_estimators} sont systématiquement enregistrés, de même que toutes les métriques d'évaluation calculées sur le jeu de test. La matrice de confusion et les artefacts du modèle (fichiers joblib) sont également loggés, créant ainsi un historique complet et traçable de chaque itération.

L'interface graphique MLflow UI, accessible via la commande \texttt{mlflow ui}, permet de visualiser et comparer rapidement plusieurs runs, de suivre l'évolution des performances au fil du temps et de sélectionner la meilleure configuration de modèle à promouvoir vers la production.

\subsection{Gestion des artefacts}

Les artefacts générés par le pipeline comprennent le modèle sérialisé au format joblib, le vectoriseur TF--IDF également sérialisé, les métriques d'évaluation en format JSON, les rapports textuels détaillés et les visualisations comme la matrice de confusion.

Ces artefacts suivent un double circuit de persistance. D'une part, ils sont sauvegardés dans des répertoires dédiés du dépôt (\texttt{models/} et \texttt{reports/}) pour être facilement accessibles et intégrables à l'image Docker. D'autre part, ils sont loggés dans MLflow pour conserver un historique versionné et faciliter la comparaison entre différentes versions du modèle.

Cette organisation permet de reprendre facilement des expériences antérieures, de comprendre l'évolution des performances et de simplifier le déploiement du modèle sélectionné en production.

%========================
% 6. Service d'inférence et packaging
%========================
\section{Service d'inférence et packaging}

\subsection{API FastAPI}

Le service de prédiction est implémenté dans \texttt{src/app.py} en utilisant le framework FastAPI, reconnu pour ses performances et sa facilité d'utilisation. L'application utilise un gestionnaire de cycle de vie (\texttt{lifespan}) pour charger les artefacts nécessaires au démarrage : le modèle entraîné, le vectoriseur TF--IDF, les stopwords anglais et le lemmatiseur NLTK.

L'API expose deux endpoints principaux. Le premier, \texttt{GET /health}, retourne un statut \texttt{ok} accompagné d'une indication sur le chargement correct du modèle, permettant aux orchestrateurs de vérifier la disponibilité du service. Le second, \texttt{POST /predict}, accepte un payload JSON contenant un champ \texttt{text} avec le document à classifier. Le texte est prétraité avec les mêmes fonctions que celles utilisées lors de l'entraînement, garantissant la cohérence du pipeline, puis vectorisé et soumis au modèle.

La réponse retournée contient le texte original, l'identifiant de la classe prédite (\texttt{prediction\_class\_id}) et un champ \texttt{status} indiquant le succès de l'opération. Un script additionnel \texttt{src/predict.py} offre une interface en ligne de commande pour effectuer des prédictions sans passer par l'API.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{snapshots/fastapi-predict.png}
  \caption{Exemple de requête et réponse de l'API FastAPI}
\end{figure}

\subsection{Containerisation Docker}

Le \texttt{Dockerfile} à la racine du projet définit l'image de service encapsulant l'API. Cette image est construite à partir de \texttt{python:3.9-slim}, une image légère offrant un bon compromis entre taille et fonctionnalités.

Le processus de build installe d'abord les dépendances Python spécifiées dans \texttt{requirements.txt}, puis télécharge les ressources NLTK nécessaires (tokenizers, stopwords, lemmatiseur) directement pendant la construction de l'image. Cette approche évite les téléchargements au runtime et garantit que le service démarre rapidement et de manière fiable, même sans accès réseau.

Le code source, les artefacts du modèle et les données prétraitées sont ensuite copiés dans l'image. Le port 8000 est exposé et l'application est lancée via Uvicorn, un serveur ASGI performant adapté aux applications FastAPI. Cette containerisation garantit un environnement d'exécution reproductible sur n'importe quelle machine ou plateforme cloud supportant Docker.

%========================
% 7. Stratégie de tests et validation
%========================
\section{Stratégie de tests et validation}

\subsection{Tests unitaires et d'intégration}

Le répertoire \texttt{tests/} contient une suite de tests automatisés couvrant les différents composants du projet. Les tests des fonctions de prétraitement vérifient le bon fonctionnement du nettoyage du texte, de la tokenisation, de la suppression des stopwords et de la lemmatisation, garantissant que les transformations appliquées aux données sont correctes et cohérentes.

Les tests du pipeline de données s'assurent que les fichiers \texttt{train.csv}, \texttt{validation.csv} et \texttt{test.csv} sont correctement générés et que les artefacts du modèle sont présents après l'exécution du script d'entraînement. Les tests du service d'API vérifient les endpoints \texttt{/health} et \texttt{/predict}, contrôlant les statuts HTTP retournés, la structure des réponses JSON et la gestion appropriée des entrées invalides.

L'ensemble de ces tests est exécuté automatiquement dans les workflows CI avant la construction de l'image Docker, bloquant le processus en cas d'échec et garantissant ainsi un niveau minimal de qualité avant tout déploiement.

\subsection{Validation de modèle}

Au-delà des tests techniques, la validation du modèle s'appuie sur plusieurs mécanismes complémentaires. Les métriques quantitatives loggées dans MLflow (accuracy, F1-score pondéré, etc.) permettent de suivre l'évolution des performances et de détecter toute régression. Les artefacts d'analyse comme la matrice de confusion et le rapport de classification offrent une compréhension fine du comportement du modèle sur chaque classe.

Un seuil d'accuracy minimal, configuré dans le workflow de déploiement via la variable \texttt{MIN\_ACCURACY}, conditionne la promotion du modèle en production. Si les performances observées en staging ne dépassent pas ce seuil, le déploiement est automatiquement bloqué et un rollback est déclenché. Cette validation automatisée réduit significativement le risque de déployer un modèle dégradé en production.

%========================
% 8. Pipelines CI/CD et déploiement
%========================
\section{Pipelines CI/CD et déploiement}

\subsection{Build, tests et image Docker}

Le workflow \texttt{.github/workflows/docker.yaml} implémente une pipeline d'intégration continue centrée sur la construction de l'image de service. Ce workflow se déclenche automatiquement sur chaque push et pull request vers la branche \texttt{master}.

Le processus commence par l'installation de Python 3.9 et des dépendances du projet via \texttt{requirements.txt}. Les scripts \texttt{src/preprocess.py} et \texttt{src/train.py} sont ensuite exécutés séquentiellement pour régénérer les données prétraitées et les artefacts du modèle à partir de zéro, garantissant ainsi la reproductibilité complète du build. La suite de tests est lancée via \texttt{pytest tests/ -v}.

Si tous les tests passent, le workflow se connecte au GitHub Container Registry (\texttt{ghcr.io}) et construit l'image Docker à partir du \texttt{Dockerfile}. Sur les branches non-pull\_request, l'image est publiée avec les tags \texttt{latest} et le SHA du commit, permettant un versioning précis des images déployées.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{snapshots/docker_build.png}
  \caption{Diagramme du workflow de build et construction de l'image Docker}
\end{figure}

\subsection{Rapport automatique avec CML}

Le workflow \texttt{.github/workflows/cml.yaml} exploite l'outil CML (Continuous Machine Learning) développé par Iterative pour générer des rapports expérimentaux automatiques directement dans les pull requests. Après l'installation des dépendances et l'exécution du pipeline complet de prétraitement et d'entraînement, le workflow construit un fichier \texttt{report.md} synthétisant les résultats.

Ce rapport contient les métriques d'évaluation au format JSON extraites de \texttt{metrics.json}, le rapport de classification détaillé par classe issu de \texttt{classification\_report.txt} et la matrice de confusion publiée comme image via la commande \texttt{cml publish}. Le rapport complet est automatiquement posté en commentaire sur la pull request via \texttt{cml comment create}, offrant une visibilité immédiate sur l'impact des modifications proposées.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{snapshots/cml_workflow.png}
  \caption{Diagramme du workflow de génération automatique du rapport CML}
\end{figure}

\subsection{Déploiement staging vers production et rollback}

Le workflow \texttt{.github/workflows/deploy.yaml} met en place une stratégie de déploiement progressif en deux phases avec mécanisme de rollback automatique. Ce workflow se déclenche à la complétion réussie du workflow Docker Build \& Push sur la branche \texttt{master}.

La première phase, le job \texttt{deploy-staging}, récupère la dernière image Docker depuis le registre et lance un conteneur en environnement staging sur le port 8000. Des tests d'intégration simples sont effectués via \texttt{curl} sur les endpoints \texttt{/health} et \texttt{/predict}, vérifiant que le modèle est correctement chargé et fonctionnel.

La seconde phase, le job \texttt{deploy-production}, ne s'exécute que si le staging a réussi. Elle vérifie que l'accuracy observée dépasse le seuil minimum configurable \texttt{MIN\_ACCURACY} (par défaut 0.5), puis procède au déploiement de l'image en production. Cette phase marque également la version comme release de production pour faciliter les audits.

En cas d'échec à n'importe quelle étape, le job \texttt{rollback} se déclenche automatiquement. Il journalise les actions de rollback prévues, restaure la version précédente stable et crée un ticket GitHub documentant l'incident pour analyse ultérieure.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{snapshots/deploy_pipeline.png}
  \caption{Diagramme du workflow de déploiement staging vers production avec rollback}
\end{figure}

%========================
% 9. Monitoring et exploitation
%========================
\section{Monitoring et exploitation}

Le monitoring du service en production repose actuellement sur plusieurs mécanismes complémentaires. Les endpoints \texttt{/health} et \texttt{/predict} sont utilisés dans les tests d'intégration du workflow de déploiement pour vérifier que le service est opérationnel et que le modèle est correctement chargé. Ces vérifications constituent une forme de smoke test automatisé à chaque déploiement.

Les métriques et artefacts loggés dans MLflow permettent un suivi historique des performances du modèle au fil des différentes versions entraînées. Ce suivi est essentiel pour détecter des régressions de performance et comprendre l'évolution du modèle dans le temps. Les rapports CML générés dans les pull requests résument les performances de chaque nouvelle version du modèle, offrant une visibilité continue sur la qualité du système.

Cependant, en l'état actuel, il n'existe pas encore de monitoring runtime complet en production. La collecte de métriques opérationnelles (latence des requêtes, taux d'erreur, utilisation des ressources), la détection de dérive des données d'entrée et les alertes automatiques en cas d'anomalie sont identifiées comme des axes d'amélioration majeurs pour une industrialisation complète du projet.

%========================
% 10. Limites et axes d'amélioration
%========================
\section{Limites et axes d'amélioration}

\subsection{Limites du modèle et des données}

La principale limite du modèle réside dans le choix d'une approche classique basée sur TF--IDF et Random Forest. Bien que cette combinaison soit simple à implémenter et offre des performances raisonnables, elle ne tire pas parti des avancées récentes en traitement du langage naturel, notamment les modèles de type transformers (BERT, RoBERTa, etc.) et les embeddings contextuels qui captent mieux les nuances sémantiques.

Les hyperparamètres du modèle (\texttt{max\_features}, \texttt{n\_estimators}, profondeur des arbres) sont actuellement fixés en dur dans le code, sans recherche automatisée de la configuration optimale. L'intégration d'une recherche Grid Search, Random Search ou d'optimisation bayésienne permettrait d'améliorer significativement les performances.

Le prétraitement linguistique reste basique : il ne prend pas en compte les bigrammes ou trigrammes qui pourraient capturer des expressions composées, ne gère pas la détection de langue pour filtrer les documents non anglais, et n'intègre pas de traitement spécifique pour les caractères spéciaux ou émojis. Enfin, la correspondance entre identifiant de classe et nom de catégorie n'est pas exposée dans l'API, compliquant l'interprétation métier des prédictions.

\subsection{Limites MLOps et CI/CD}

Le tracking MLflow est actuellement configuré en mode fichier local, ce qui fonctionne bien pour un usage individuel mais limite les possibilités en environnement distribué ou multi-utilisateurs. La migration vers un serveur MLflow centralisé permettrait une collaboration plus fluide et un suivi unifié des expériences.

Le projet ne dispose pas encore d'un véritable registry de modèles avec gestion formelle des versions et des promotions entre environnements (staging, production). L'intégration du Model Registry de MLflow ou d'un outil équivalent renforcerait la gouvernance des modèles déployés.

Le couplage actuel entre le pipeline de prétraitement/entraînement et le workflow de build rallonge les temps de CI, puisque chaque build reconstruit le modèle de zéro. Une séparation plus nette entre les pipelines de data science et les pipelines de build/déploiement optimiserait les temps d'exécution. Le seuil de performance utilisé pour la validation du déploiement est actuellement simulé avec une valeur fixe, réduisant le réalisme de la validation automatisée.

\subsection{Sécurité, robustesse et industrialisation}

La gestion des ressources NLTK au démarrage de l'application peut être fragile en contexte de production avec des contraintes réseau, bien que le Dockerfile anticipe déjà certains téléchargements lors du build. La couverture de tests, bien que couvrant les points clés, ne gère pas exhaustivement tous les cas d'erreur possibles : entrées invalides, timeouts, modèles manquants, etc.

La configuration du projet (chemins, hyperparamètres, URI MLflow) est actuellement codée en dur dans le code source. Une externalisation via des variables d'environnement ou des fichiers de configuration (YAML, TOML) renforcerait la flexibilité et faciliterait le déploiement sur différents environnements. Enfin, le job de déploiement reste illustratif, avec des placeholders pour des intégrations futures vers Kubernetes ou des déploiements SSH, sans connexion à une infrastructure réelle.

%========================
% 11. Conclusion
%========================
\section{Conclusion}

Ce projet illustre la mise en place d'un pipeline MLOps cohérent de bout en bout pour un cas d'usage de classification de texte. L'architecture développée couvre l'ensemble du cycle de vie du modèle : préparation des données textuelles, entraînement et évaluation du modèle, suivi expérimental avec MLflow, exposition via une API FastAPI, containerisation Docker et automatisation CI/CD avec rapports CML et déploiement progressif staging vers production.

Les choix techniques réalisés privilégient la simplicité et la pédagogie : Random Forest combiné à TF--IDF pour la modélisation, MLflow en mode local pour le tracking, GitHub Actions pour l'orchestration CI/CD et CML pour la génération de rapports automatiques. Ces choix offrent une base solide et accessible pour explorer les bonnes pratiques MLOps tout en restant compréhensibles pour des équipes découvrant ces concepts.

Les principales pistes d'amélioration identifiées concernent la modernisation du modèle vers des architectures de type transformers, la mise en place d'un tracking MLflow centralisé et d'un véritable registry de modèles, l'enrichissement de la couverture de tests et du monitoring en production, ainsi que la décorrélation des pipelines de data science et des pipelines de build/déploiement pour optimiser les temps d'exécution et la maintenabilité globale du système.

\end{document}
